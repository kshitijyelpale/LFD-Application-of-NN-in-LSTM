{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Custom_Dataset.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzvaR8rXXpf4",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_kTGyOBXiFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import nltk\n",
        "from google.colab import drive\n",
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, model_from_json\n",
        "from keras.layers import Dense, Embedding\n",
        "from keras.layers import LSTM\n",
        "try:\n",
        "    from nltk.corpus import stopwords\n",
        "except:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjCZorGZZrAj",
        "colab_type": "text"
      },
      "source": [
        "Method for Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qH-mVadMZtk-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_data(path, x_data, y_data):\n",
        "\n",
        "  for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename), 'r', encoding=\"utf8\") as f:\n",
        "            st_ind = list(filename).index(\"_\")\n",
        "            nd_ind = list(filename).index(\".\")\n",
        "            if filename[nd_ind - 1] == ')':\n",
        "                nd_ind -= 4 \n",
        "            x_data.append(f.read())\n",
        "            y_data.append(int(filename[st_ind + 1:nd_ind]) / 10)\n",
        "\n",
        "  return x_data, y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvfmcYUUXucg",
        "colab_type": "text"
      },
      "source": [
        "Method for importing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUYJEL9PX0Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def import_data():\n",
        "\n",
        "    print(\"Importing Dataset... Please wait!\")\n",
        "\n",
        "    dataset_path = \"/content/gdrive/My Drive/Colab Notebooks/DataSet/\"\n",
        "\n",
        "    test_data_path = dataset_path + \"Test/\"\n",
        "    pos_test_data_path = test_data_path + \"Positive\"\n",
        "    neg_test_data_path = test_data_path + \"Negative\"\n",
        "\n",
        "    train_data_path = dataset_path + \"Train/\"\n",
        "    pos_train_data_path = train_data_path + \"Positive\"\n",
        "    neg_train_data_path = train_data_path + \"Negative\"\n",
        "\n",
        "    x_train, y_train = read_data(neg_train_data_path, [], [])\n",
        "    x_train, y_train = read_data(pos_train_data_path, x_train, y_train)\n",
        "\n",
        "    x_test, y_test = read_data(neg_test_data_path, [], [])\n",
        "    x_test, y_test = read_data(pos_test_data_path, x_test, y_test)\n",
        "\n",
        "\n",
        "    print(\"Dataset successfully imported...!!!\")\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ew92PIobfzc",
        "colab_type": "text"
      },
      "source": [
        "Method for Data Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5MZ6Hzrbpqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preProcessData(x_data):\n",
        "        \n",
        "        print(\"Pre-processing data... Please wait!\")\n",
        "        \n",
        "        #Fetch all stopwords and keep required stopwords\n",
        "        all_stopwords = stopwords.words('english')\n",
        "        my_stopwords = [ word for word in all_stopwords if word not in (\"against\", \"up\", \"down\", \"out\", \"off\", \"over\", \"under\", \"more\", \"most\", \"each\", \"few\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"too\", \"very\", \"don\", \"don't\", 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\")]\n",
        "        \n",
        "        #Get rid of special characters\n",
        "        REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
        "        REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
        "        \n",
        "        for i in range(0,len(x_data)):\n",
        "            #Keep only alphabets with single whitespace\n",
        "            x_data[i] = REPLACE_NO_SPACE.sub(\"\", x_data[i].lower())\n",
        "            x_data[i] = REPLACE_WITH_SPACE.sub(\" \", x_data[i])\n",
        "            \n",
        "            #Remove unwanted stopwords\n",
        "            x_data[i] = x_data[i].split()\n",
        "            x_data[i] = [ word for word in x_data[i] if word not in my_stopwords]\n",
        "            x_data[i] = \" \".join(x_data[i])\n",
        " \n",
        "        print(\"Pre-processing done...!!!\")\n",
        "        return x_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2S5yKFFcJPr",
        "colab_type": "text"
      },
      "source": [
        "Method for Word Embedding - One-Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsl2eBFdcPtU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encodeData(max_features, max_doc_len, x_data):\n",
        "        \n",
        "        print(\"Encoding data to One Hot Representation...\")\n",
        "        \n",
        "        x_data = [ one_hot(document, max_features) for document in x_data]\n",
        "        \n",
        "        #Add Bias\n",
        "        for i in range(0, len(x_data)):\n",
        "            x_data[i] = [1] + x_data[i]\n",
        "        \n",
        "        #Word Embedding\n",
        "        x_data = pad_sequences(x_data, truncating = 'post', padding = 'post', maxlen = max_doc_len)\n",
        "        \n",
        "        print(\"One-Hot Representation Encoding done...!!!\")\n",
        "        return x_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXgLd5WSc44u",
        "colab_type": "text"
      },
      "source": [
        "Method for creating ML->RNN->LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byAEy7O0c9e5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel(max_features):\n",
        "        \n",
        "        print(\"Creating model...\")\n",
        "\n",
        "        model = Sequential()\n",
        "        #Layer 1-> Embedding\n",
        "        model.add(Embedding(max_features, 128))     // 32 * 200 * 128\n",
        "        #Layer 2-> LSTM\n",
        "        model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))    // 32 * 128 \n",
        "        #Layer 3-> Fully Connected (Dense)\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        #Choose best optimizer\n",
        "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "        \n",
        "        print(\"Model created...!!!\")\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmVxYtcxdPhR",
        "colab_type": "text"
      },
      "source": [
        "Method for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yPeVg4pdTjs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainModel(model, x_train, y_train, x_test, y_test):\n",
        "        \n",
        "        print(\"Training Model... Please wait!\")\n",
        "\n",
        "        model.fit(x_train, y_train, batch_size=32, epochs=15, validation_data=(x_test, y_test))\n",
        "        \n",
        "        print(\"Model trained...!!!\")\n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a21OjdBBdbxX",
        "colab_type": "text"
      },
      "source": [
        "Method for evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXI8LlKldgi5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validateModel(model, x_test, y_test):\n",
        "\n",
        "        score, acc = model.evaluate(x_test, y_test, batch_size=32)\n",
        "        print('Test score:', score)\n",
        "        print('Test accuracy:', acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMSlpLFvdojg",
        "colab_type": "text"
      },
      "source": [
        "Method for Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrMUGVUGdrG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predictModel(model, x_data):\n",
        "\n",
        "  print(model.predict(x_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96bAxtNOd6Cv",
        "colab_type": "text"
      },
      "source": [
        "Method for saving trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCYtQ6qGeAFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def saveModel(model):\n",
        "\n",
        "        #Serialize to JSON\n",
        "        json_file = model.to_json()\n",
        "        with open(\"Model_LSTM.json\", \"w\") as file:\n",
        "            file.write(json_file)\n",
        "        \n",
        "        #Serialize weights to HDF5\n",
        "        model.save_weights(\"lstm_model_weights.h5\")\n",
        "        \n",
        "        print(\"Model saved...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bSnhxn0eW6H",
        "colab_type": "text"
      },
      "source": [
        "Method for loading saved model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnAZ7zygebqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadModel():\n",
        "\n",
        "        #Load JSON and create model\n",
        "        file = open(\"Model_LSTM.json\", \"r\")\n",
        "        model_json = file.read()\n",
        "        file.close()\n",
        "        \n",
        "        model = model_from_json(model_json)\n",
        "        #Load weights\n",
        "        model.load_weights(\"lstm_model_weights.h5\")\n",
        "        print(\"Model loaded successfully...\")\n",
        "        \n",
        "        return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLHRbA4feqhf",
        "colab_type": "text"
      },
      "source": [
        "Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwUv-1L6eun3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "\n",
        "  #Define vocabulary size\n",
        "  max_features = 47000\n",
        "  #Define number of words per document/review\n",
        "  max_doc_len = 220\n",
        "\n",
        "  x_train, y_train, x_test, y_test = import_data()\n",
        "\n",
        "  x_train = preProcessData(x_train)\n",
        "  x_test = preProcessData(x_test)\n",
        "\n",
        "  x_train = encodeData(max_features, max_doc_len, x_train)\n",
        "  x_test = encodeData(max_features, max_doc_len, x_test)\n",
        "\n",
        "  model = createModel(max_features)\n",
        "\n",
        "  model = trainModel(model, x_train, y_train, x_test, y_test)\n",
        "\n",
        "  validateModel(model, x_test, y_test)\n",
        "\n",
        "  saveModel(model)\n",
        "\n",
        "  model = loadModel()\n",
        "\n",
        "  # Predict new document\n",
        "  new_document = [\"\"\"I absolutely adored this movie. For me, the best reason to see it is how stark a contrast it is from legal dramas like \"Boston Legal\" or \"Ally McBeal\" or even \"LA Law.\" This is REALITY. The law is not BS, won in some closing argument or through some ridiculous defense you pull out of your butt, like the \"Chewbacca defense\" on South Park.) This is a real travesty of justice, the legal system gone horribly wrong, and the work by GOOD lawyers - not the shyster stereotype, who use all of their skills to right it. It will do more for restoring your faith in humanity than any Frank Capra movie or TO KILL A MOCKINGBIRD. And most importantly, I wept. During the film, during the featurette included at the end of the DVD - it's amazing. Wonderful film; wonderfully made. Thank God the filmmakers made it.\"\"\"]\n",
        "\n",
        "  new_document = preProcessData(new_document)\n",
        "\n",
        "  new_document = encodeData(max_features, max_doc_len, new_document)\n",
        "\n",
        "  predictModel(model, new_document)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFi2rQKBhDRl",
        "colab_type": "text"
      },
      "source": [
        "Call Main Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxv4IlPHhFXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "outputId": "7e0303f2-a932-4d38-ada6-debb697a0d43"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Importing Dataset... Please wait!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-41c248c53944>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mmax_doc_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m220\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreProcessData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-8a237ed7efe1>\u001b[0m in \u001b[0;36mimport_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_train_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_train_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mneg_test_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-7295b551db9c>\u001b[0m in \u001b[0;36mread_data\u001b[0;34m(path, x_data, y_data)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mst_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 5] Input/output error: '/content/gdrive/My Drive/Colab Notebooks/DataSet/Train/Positive'"
          ]
        }
      ]
    }
  ]
}